{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderVGG16(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_layers = 5,\n",
    "                 h_dims = [512,256,128,10]):\n",
    "        super(EncoderVGG16,self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.h_dims = h_dims\n",
    "        self.layers = [4,9,16,23,31]\n",
    "        self.full_vgg16 = torchvision.models.vgg16()\n",
    "        self.vgg = self.full_vgg16.features[:self.layers[self.n_layers-1]]\n",
    "        self.classifier = self.full_vgg16.classifier\n",
    "        \n",
    "        self.classifier[0] = nn.Linear(self.h_dims[0], self.h_dims[1])\n",
    "        self.classifier[3] = nn.Linear(self.h_dims[1], self.h_dims[2])\n",
    "        self.classifier[6] = nn.Linear(self.h_dims[2], self.h_dims[3])\n",
    "    \n",
    "    def encode(self,x):\n",
    "        return self.vgg(x)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        latent = self.encode(x)\n",
    "        latent = torch.flatten(latent,start_dim = 1)\n",
    "        output = self.classifier(latent)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderVGG16()\n",
    "# model.vgg, model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderVGG16(\n",
       "  (full_vgg16): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (vgg): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "    print(\"Data directory created :D\")\n",
    "if not os.path.exists(\"data/train\"):\n",
    "    os.makedirs(\"data/train\")\n",
    "    print(\"Data train directory created :D\")\n",
    "\n",
    "if not os.path.exists(\"data/val\"):\n",
    "    os.makedirs(\"data/val\")\n",
    "    print(\"Data val directory created :D\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "     # transforms.Resize(size=(224, 224)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_datset = torchvision.datasets.CIFAR10(root = \"data/train/\",train=True, transform= transform,download=True)\n",
    "val_dataset = torchvision.datasets.CIFAR10(root = \"data/val/\",train = False, transform= transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: data/train/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data/val/\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoaders y criterios de optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_datset, batch_size=4,shuffle=True,)\n",
    "valloader = DataLoader(val_dataset,batch_size= 4,shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),weight_decay= 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, cuda):\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Batch {i}/{len(train_loader)}\")\n",
    "\n",
    "        inputs, labels = data\n",
    "\n",
    "        \n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # del loss, data  \n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        train_loss += loss.item()\n",
    "    accuracy = correct/total\n",
    "    train_loss /= len(train_loader)\n",
    "    return train_loss, accuracy\n",
    "    \n",
    "def validate(model, val_loader, criterion, cuda):\n",
    "    \n",
    "    correct = 0\n",
    "    val_loss = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "\n",
    "            inputs, labels = data\n",
    "\n",
    "            if cuda:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs,labels)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            del loss, data  \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = correct/total\n",
    "    return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, \n",
    "             epochs, \n",
    "             train_loader, \n",
    "             val_loader, \n",
    "             criterion, \n",
    "             optimizer, \n",
    "             each = 50, \n",
    "             state = None,\n",
    "             name = \"\",\n",
    "             cuda = True):\n",
    "    \n",
    "    # If trained before\n",
    "    if state == None:\n",
    "        state = {\n",
    "            \"epoch\" : 0,\n",
    "            \"loss\" : [[], []], # [train, val]\n",
    "            \"acc\" : [[], []], # [train, val]\n",
    "            \"params\" : None, \n",
    "            \"bestloss\" : np.inf\n",
    "        }\n",
    "\n",
    "    # If previously trained\n",
    "    else:\n",
    "        state = torch.load(state)\n",
    "        model.load_state_dict(state[\"params\"])\n",
    "\n",
    "    best_loss = state[\"bestloss\"]\n",
    "    state_epochs = state[\"epoch\"]\n",
    "\n",
    "    if cuda:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    for epoch in range(state_epochs, state_epochs + epochs):\n",
    "        print(f\"Epoch nro {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, cuda)\n",
    "        # Val\n",
    "        val_loss, val_acc = validate(model, val_loader,criterion, cuda)\n",
    "       \n",
    "        print(f\"Train loss = {train_loss}, Val loss = {val_loss}\")\n",
    "        print(f\"Train acc = {train_acc}, Val acc = {val_acc}\")\n",
    "        # print(f\"Val acc = {val_acc}\")\n",
    "\n",
    "        if (best_loss > val_loss):\n",
    "            best_loss = val_loss\n",
    "            print(f\"Better params found in epoch = {epoch + 1}, saved params\")\n",
    "            torch.save(model.state_dict(), f'bestVGGParams{name}.pt')\n",
    "        # Load best model so far to proceed\n",
    "    \n",
    "        # Save periodically for each\n",
    "        if ((epoch + 1)%each == 0):\n",
    "            print(f\"Se ha guardado la época múltiplo de {each}\")\n",
    "            torch.save(model.state_dict(), f'eachVGGParams_{epoch + 1}.pt')\n",
    "\n",
    "        # Update state\n",
    "        state[\"loss\"][0].append(train_loss)\n",
    "        state[\"loss\"][1].append(val_loss)\n",
    "        state[\"acc\"][0].append(train_acc)\n",
    "        state[\"acc\"][1].append(val_acc)\n",
    "        state[\"epoch\"] = epoch + 1\n",
    "        state[\"params\"] = model.state_dict()\n",
    "        state[\"bestloss\"] = best_loss\n",
    "\n",
    "        # Save per epoch just in case\n",
    "        torch.save(state, f\"VGG_{epoch + 1}.pt\")\n",
    "    return state[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state = training(model = model,\n",
    "                     epochs = 2,\n",
    "                     train_loader = trainloader,\n",
    "                     val_loader = valloader,\n",
    "                     criterion = criterion,\n",
    "                     optimizer = optimizer,\n",
    "                     each = 50,\n",
    "                     state = None,\n",
    "                     name = \"\",\n",
    "                     cuda = True,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
